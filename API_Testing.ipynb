{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2EJpTKN6WgK",
        "outputId": "0463d924-c19c-4a06-d8fa-7ee9c770cb61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNxA0PGf6Z3v",
        "outputId": "774dfcf5-a716-4adc-e0a7-5a9c878a8eed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fast inference is critical for reasoning models for several reasons:\n",
            "\n",
            "1. **Real-time Decision Making**: Many applications that utilize reasoning models, such as autonomous vehicles, robots, or real-time expert systems, require fast and timely decision-making. Slow inference can lead to delayed or incorrect decisions, which can have severe consequences.\n",
            "2. **User Experience**: In interactive applications, such as virtual assistants or chatbots, slow inference can lead to a poor user experience. Users expect rapid responses to their queries, and delays can cause frustration and abandonment.\n",
            "3. **Scalability**: As the complexity of reasoning models increases, the computational requirements for inference also grow. Fast inference enables these models to be deployed on a larger scale, handling more users, data, and scenarios without becoming bottlenecked.\n",
            "4. **Energy Efficiency**: Fast inference can lead to significant energy savings, especially in battery-powered devices or edge computing applications. By reducing the computational time, devices can conserve energy and prolong their battery life.\n",
            "5. **Competitive Advantage**: In many industries, such as finance, healthcare, or logistics, fast inference can provide a competitive advantage. Organizations that can process and respond to data quickly can react faster to changing circumstances, make better decisions, and stay ahead of their competitors.\n",
            "6. **Limited Resources**: In many scenarios, such as edge computing, mobile devices, or embedded systems, computational resources are limited. Fast inference is essential to ensure that reasoning models can operate within these constraints, providing accurate and reliable results.\n",
            "7. **Increased Model Complexity**: As reasoning models become more complex and sophisticated, fast inference is necessary to support the increased computational demands. This enables researchers to develop more accurate and robust models without being limited by inference speed.\n",
            "\n",
            "To achieve fast inference, reasoning models can leverage various techniques, such as:\n",
            "\n",
            "1. **Model pruning**: Removing unnecessary weights and connections to reduce computational complexity.\n",
            "2. **Knowledge distillation**: Transferring knowledge from complex models to simpler ones, reducing inference time.\n",
            "3. **Quantization**: Representing model weights and activations using lower-precision data types, reducing computational requirements.\n",
            "4. **Parallelization**: Distributing inference computations across multiple processing units, such as GPUs or TPUs.\n",
            "5. **Compilation**: Compiling reasoning models into optimized, platform-specific code, reducing inference time.\n",
            "6. **Approximation methods**: Using approximation techniques, such as sampling or sparse coding, to reduce computational complexity.\n",
            "7. **Specialized hardware**: Utilizing custom-designed hardware, such as ASICs or FPGAs, optimized for fast inference.\n",
            "\n",
            "By prioritizing fast inference, researchers and developers can create more efficient, scalable, and effective reasoning models that can be deployed in a wide range of applications, from autonomous systems to intelligent assistants.\n"
          ]
        }
      ],
      "source": [
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "GROQ_API_KEY = userdata.get('grok_api_key')\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain why fast inference is critical for reasoning models\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cacku9v_3MsT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
